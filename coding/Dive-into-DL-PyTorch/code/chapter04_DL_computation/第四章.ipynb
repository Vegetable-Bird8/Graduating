{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c772e3f",
   "metadata": {},
   "source": [
    "# 4.1模型构造"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1ddbdc",
   "metadata": {},
   "source": [
    "Module 类是 nn 模块⾥提供的⼀个模型构造类，是所有神经⽹络模块的基类，我们可以继承它来定义我们想要的模型。下⾯继承 Module 类构造本节开头提到的多层感知机。这⾥定义的 MLP 类᯿载了Module 类的 __init__ 函数和 forward 函数。它们分别⽤于创建模型参数和定义前向计算。前向计\n",
    "算也即正向传播。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "112e212c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    # 声明带有模型参数的层  这里声明了两个全连接层\n",
    "    def __init__(self,**kwargs):\n",
    "        # 调用MLP父类Block的构造函数来进行必要的初始化\n",
    "        # 这样在构造实例时还可以指定其他函数\n",
    "        super(MLP,self).__init__(**kwargs)\n",
    "        self.hidden = nn.Linear(784,256)  #隐藏层\n",
    "        self.act = nn.ReLU()\n",
    "        self.output = nn.Linear(256,10)  #输出层\n",
    "        \n",
    "        # 定义前向计算  \n",
    "    def forward(self,x):\n",
    "        a = self.act(self.hidden(x))\n",
    "        return self.output(a)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdddde8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (act): ReLU()\n",
      "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(2,784)\n",
    "net = MLP()\n",
    "print(net)\n",
    "b = net(X)\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd60084",
   "metadata": {},
   "source": [
    "## module 的子类\n",
    "### Sequential类\n",
    "它可以接收⼀个⼦模块的有序字典（OrderedDict）或者⼀系列⼦模块作\n",
    "为参数来逐⼀添加 Module 的实例，⽽模型的前向计算就是将这些实例按添加的顺序逐⼀计算。\n",
    "下⾯我们实现⼀个与 Sequential 类有相同功能的 MySequential 类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "265fc401",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySequential(nn.Module):\n",
    "    from collections import OrderedDict\n",
    "    def __init__(self,*args):\n",
    "        super(MySequential,self).__init__()\n",
    "        if len(args) == 1 and isinstance(args[0],OrderedDict):\n",
    "            #如果传入的是一个OrderedDict\n",
    "            for key,module in args[0].items():\n",
    "                self.add_module(key,module)\n",
    "                # add_module⽅法会将module添加进self._modules(⼀个OrderedDict)\n",
    "        else: #如果传入的是一些Module\n",
    "            for idx,module in enumerate(args):\n",
    "                self.add_module(str(idx),module)\n",
    "    def forward(self,input):\n",
    "        #self._module返回一个OrderedDict \n",
    "        # 保证会按照成员添加是的顺序遍历\n",
    "        for module in self._modules.values():\n",
    "            input = module(input)\n",
    "        return input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be34cbc5",
   "metadata": {},
   "source": [
    "Sequential ModuleList ModuleDict 都可以方便的添加神经网络层 且不需要添加forward函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfb7f9f",
   "metadata": {},
   "source": [
    "## 4.1.3 构造复杂的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06750d8b",
   "metadata": {},
   "source": [
    "虽然上⾯介绍的这些类可以使模型构造更加简单，且不需要定义 forward 函数，但直接继承 Module类可以极⼤地拓展模型构造的灵活性。\n",
    "下⾯我们构造⼀个稍微复杂点的⽹络 FancyMLP 。在这个⽹络中，我们通过 get_constant 函数创建训练中不被迭代的参数，即常数参数。\n",
    "在前向计算中，除了使⽤创建的常数参数外，我们还使⽤ Tensor 的函数和Python的控制流，并多次调⽤相同的层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6197a6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FancyMLP(nn.Module):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(FancyMLP,self).__init__(**kwargs)\n",
    "        \n",
    "        self.rand_weight = torch.rand((20,20),requires_grad=False)  \n",
    "        # 不可训练参数 常数参数\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "    def forward(self,x):\n",
    "        x = self.linear(x)\n",
    "        # 使用创建的常数参数 以及nn.functional中的relu函数和mm函数\n",
    "        x = nn.functional.relu(torch.mm(x,self.rand_weight.data) +1)\n",
    "        # 复⽤全连接层。等价于两个全连接层共享参数\n",
    "        x = self.linear(x)\n",
    "        # 控制流 这里我们需要调用item函数来返回标量进行比较\n",
    "        while x.norm().item() >1:\n",
    "#             print(x.norm().item())\n",
    "            x/=2\n",
    "        if x.norm().item()<0.8:\n",
    "            x*=10\n",
    "        return x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "660d706d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FancyMLP(\n",
      "  (linear): Linear(in_features=20, out_features=20, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-6.5763, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(2, 20)\n",
    "net = FancyMLP()\n",
    "print(net)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5cf7fe",
   "metadata": {},
   "source": [
    "因为 FancyMLP 和 Sequential 类都是 Module 类的⼦类，所以我们可以嵌套调⽤它们。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05a4e4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): NestMLP(\n",
      "    (net): Sequential(\n",
      "      (0): Linear(in_features=40, out_features=30, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=30, out_features=20, bias=True)\n",
      "  (2): FancyMLP(\n",
      "    (linear): Linear(in_features=20, out_features=20, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(NestMLP,self).__init__(**kwargs)\n",
    "        self.net = nn.Sequential(nn.Linear(40,30),nn.ReLU())\n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "net = nn.Sequential(NestMLP(),nn.Linear(30,20),FancyMLP())\n",
    "X= torch.rand(2,40)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c5c689f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.8350, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5ec05f",
   "metadata": {},
   "source": [
    "# 4.2 模型参数的访问 初始化和共享"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "20427b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=4, out_features=3, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "net = nn.Sequential(nn.Linear(4, 3), nn.ReLU(), nn.Linear(3, 1)) #\n",
    "# pytorch已进⾏默认初始化\n",
    "print(net) \n",
    "X = torch.rand(2, 4) \n",
    "Y = net(X).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b66f20",
   "metadata": {},
   "source": [
    "## 4.2.1访问模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "39ed27b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'generator'>\n"
     ]
    }
   ],
   "source": [
    "print(type(net.named_parameters())) \n",
    "#以迭代器的形式返回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "86ec0bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight torch.Size([3, 4])\n",
      "0.bias torch.Size([3])\n",
      "2.weight torch.Size([1, 3])\n",
      "2.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for name, param in net.named_parameters():\n",
    "     print(name, param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3fed78ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight torch.Size([3, 4])\n",
      "bias torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "for name, param in net[0].named_parameters():\n",
    "     print(name, param.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2221d235",
   "metadata": {},
   "source": [
    "param 的 类 型为 torch.nn.parameter.Parameter \n",
    "其实这是 Tensor 的⼦类，\n",
    "和 Tensor 不同的是如果⼀个 Tensor 是 Parameter \n",
    "那么它会⾃动被添加到模型的参数列表⾥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f43003fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight1\n"
     ]
    }
   ],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(MyModel,self).__init__(**kwargs)\n",
    "        self.weight1 = nn.Parameter(torch.rand(20,20))\n",
    "        self.weight2 = torch.rand(20,20)\n",
    "    def forward(self,x):\n",
    "        pass\n",
    "n = MyModel()\n",
    "for name,param in n.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9892d065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.4576,  0.3284, -0.4134, -0.1837],\n",
      "        [-0.2020,  0.0932,  0.2688,  0.3181],\n",
      "        [ 0.2389, -0.3934, -0.3446,  0.2894]], requires_grad=True)\n",
      "tensor([[ 0.4576,  0.3284, -0.4134, -0.1837],\n",
      "        [-0.2020,  0.0932,  0.2688,  0.3181],\n",
      "        [ 0.2389, -0.3934, -0.3446,  0.2894]])\n",
      "tensor([[0.6855, 1.3193, 1.1676, 0.1888],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000]])\n",
      "tensor([[1.0283, 1.9789, 1.7513, 0.2831],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "weight_0 = list(net[0].parameters())[0]\n",
    "print(weight_0)\n",
    "print(weight_0.data)\n",
    "print(weight_0.grad) # 反向传播前梯度为None\n",
    "Y.backward(retain_graph=True)\n",
    "print(weight_0.grad)\n",
    "\n",
    "#  出现下述错误是因为 在经过一次计算后，梯度已经被释放\n",
    "# 因此再次计算机会找不到梯度 \n",
    "# 解决方法是在backward(retain_graph = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a1f073",
   "metadata": {},
   "source": [
    "## 4.2.2 初始化模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0ca15c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight tensor([[-0.0235,  0.0128, -0.0161,  0.0033],\n",
      "        [ 0.0134,  0.0017, -0.0179, -0.0047],\n",
      "        [ 0.0004,  0.0118,  0.0100, -0.0092]])\n",
      "tensor(-0.0180)\n",
      "2.weight tensor([[-0.0174,  0.0182, -0.0186]])\n",
      "tensor(-0.0179)\n"
     ]
    }
   ],
   "source": [
    "for name, param in net.named_parameters():\n",
    "     if 'weight' in name:\n",
    "         init.normal_(param, mean=0, std=0.01)\n",
    "         print(name, param.data)\n",
    "#          print(param.data.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "367e4389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.bias tensor([1., 1., 1.])\n",
      "2.bias tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "# 使用常数来初始化权重参数\n",
    "for name,param in net.named_parameters():\n",
    "    if 'bias' in name:\n",
    "        init.constant_(param,val = 1)\n",
    "        print(name,param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946e9e51",
   "metadata": {},
   "source": [
    "## 4.2.3 自定义初始化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4345edf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight tensor([[ 0.0000,  0.0000, -0.0000, -7.1228],\n",
      "        [-0.0000, -6.3637, -0.0000, -0.0000],\n",
      "        [-6.1078,  0.0000, -9.7701, -7.5145]])\n",
      "2.weight tensor([[-0., -0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "def normal_(temsor,mean=0,std=1):\n",
    "    with torch.no_grad():\n",
    "        return tensor.normal_(mean,std)\n",
    "\n",
    "def init_weight_(tensor):\n",
    "    with torch.no_grad():\n",
    "        tensor.uniform_(-10.10)\n",
    "        tensor *=(tensor.abs()>5).float()\n",
    "        \n",
    "for name,param in net.named_parameters():\n",
    "     if 'weight' in name:\n",
    "         init_weight_(param)\n",
    "         print(name, param.data)\n",
    "#          print(param.data.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b82d1b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.1493])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    param.uniform_(-10,10)\n",
    "    c=(param.abs()>5).float()\n",
    "param.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa85d571",
   "metadata": {},
   "source": [
    "## 4.2.4 共享模型参数\n",
    "如果我们传⼊ Sequential 的模块是同⼀个 Module 实例\n",
    "参数也是共享的，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "304aea31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=1, out_features=1, bias=False)\n",
      "  (1): Linear(in_features=1, out_features=1, bias=False)\n",
      ")\n",
      "0.weight tensor([[3.]])\n"
     ]
    }
   ],
   "source": [
    "linear = nn.Linear(1, 1, bias=False)\n",
    "net = nn.Sequential(linear, linear)\n",
    "print(net)\n",
    "for name, param in net.named_parameters():\n",
    "     init.constant_(param, val=3)\n",
    "     print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b4349c7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(id(net[0]) == id(net[1]))\n",
    "print(id(net[0].weight) == id(net[1].weight))\n",
    "# 在内存中 这两个线性层其实是一个对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dc8d4734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9., grad_fn=<SumBackward0>)\n",
      "tensor([[6.]])\n"
     ]
    }
   ],
   "source": [
    "# 因为模型参数⾥包含了梯度，\n",
    "# 所以在反向传播计算时，这些共享的参数的梯度是累加的:\n",
    "x = torch.ones(1, 1) \n",
    "y = net(x).sum()\n",
    "print(y) \n",
    "y.backward()\n",
    "print(net[0].weight.grad)# 单次梯度是3，两次所以就是6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96107e85",
   "metadata": {},
   "source": [
    "# 4.3 模型参数的延后初始化\n",
    "由于使⽤Gluon创建的全连接层的时候不需要指定输⼊个数。所以当调⽤ initialize 函数时，由于隐藏层输⼊个数依然未知，系统也⽆法得知该层权参数的形状。只有在当形状已知的输⼊ X 传进⽹络做前向计算 net(X) 时，系统才推断出该层的权重参数形状为多少，此时才进⾏真正的初始化操作。但是使⽤PyTorch在定义模型的时候就要指定输⼊的形状，所以也就不存在这个问题了"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5847da",
   "metadata": {},
   "source": [
    "# 4.4 自定义层\n",
    "## 4.4.1 不含模型参数的自定义层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d955f967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下⾯的 CenteredLayer 类通过继承 Module 类\n",
    "# ⾃定义了⼀个将输⼊减掉均值后输出的层\n",
    "# 并将层的计算定义在了 forward 函数⾥\n",
    "# 这个层⾥不含模型参数。\n",
    "\n",
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "class CenteredLayer(nn.Module):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(CenteredLayer,self).__init__(**kwargs)\n",
    "    def forward(self,x):\n",
    "        return x - x.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "51c9ed54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2., -1.,  0.,  1.,  2.])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = CenteredLayer()\n",
    "layer(torch.tensor([1,2,3,4,5],dtype = torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2d376886",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Linear(8,128),CenteredLayer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dc98bd4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.862645149230957e-09"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = net(torch.rand(4,8))\n",
    "y.mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fe9f4a",
   "metadata": {},
   "source": [
    "## 含模型参数的自定义层\n",
    "了 Parameter 类其实是 Tensor 的⼦类，如果⼀个 Tensor 是 Parameter ，那么它会⾃动被添加到模型的参数列表⾥。\n",
    "所以在⾃定义含模型参数的层时，我们应该将参数定义成 Parameter \n",
    "除了像4.2.1节那样直接定义成 Parameter 类外\n",
    "还可以使⽤ ParameterList 和 ParameterDict 分别定义参数的列表和字典。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "95af7ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyDense(\n",
      "  (params): ParameterList(\n",
      "      (0): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "      (1): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "      (2): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "      (3): Parameter containing: [torch.FloatTensor of size 4x1]\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# ParameterList 接收⼀个 Parameter 实例的列表作为输⼊然后得到⼀个参数列表\n",
    "# 使⽤的时候可以⽤索引来访问某个参数\n",
    "# 另外也可以使⽤ append 和 extend 在列表后⾯新增参数。\n",
    "\n",
    "class MyDense(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyDense,self).__init__()\n",
    "        self.params = nn.ParameterList([nn.Parameter(torch.randn(4,4)) for i in range(3)])\n",
    "        self.params.append(nn.Parameter(torch.randn(4,1)))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        for i in range(len(self.params)):\n",
    "            x = torch.mm(x,self.params[i])\n",
    "        return x\n",
    "net = MyDense()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c0989c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyDictDense(\n",
      "  (params): ParameterDict(\n",
      "      (linear1): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "      (linear2): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "      (linear3): Parameter containing: [torch.FloatTensor of size 4x2]\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MyDictDense(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyDictDense,self).__init__()\n",
    "        self.params = nn.ParameterDict({\n",
    "            'linear1':nn.Parameter(torch.randn(4,4)),\n",
    "            'linear2':nn.Parameter(torch.randn(4,4))\n",
    "        })\n",
    "        self.params.update({'linear3': nn.Parameter(torch.randn(4,2))})\n",
    "    def forward(self,x,choice = 'linear1'):\n",
    "        return torch.mm(x,self.params[choice])\n",
    "\n",
    "net = MyDictDense()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "94913421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1341,  0.0853, -2.2677, -0.0330]], grad_fn=<MmBackward>)\n",
      "tensor([[2.1894, 0.9286, 0.9847, 3.4789]], grad_fn=<MmBackward>)\n",
      "tensor([[-0.4577,  0.6231]], grad_fn=<MmBackward>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(1,4)\n",
    "print(net(x,'linear1'))\n",
    "print(net(x,'linear2'))\n",
    "print(net(x,'linear3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5157b674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): MyDictDense(\n",
      "    (params): ParameterDict(\n",
      "        (linear1): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "        (linear2): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "        (linear3): Parameter containing: [torch.FloatTensor of size 4x2]\n",
      "    )\n",
      "  )\n",
      "  (1): MyDense(\n",
      "    (params): ParameterList(\n",
      "        (0): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "        (1): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "        (2): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "        (3): Parameter containing: [torch.FloatTensor of size 4x1]\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential(\n",
    "     MyDictDense(),\n",
    "     MyDense(),\n",
    ")\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8e88fdc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-145.4057]], grad_fn=<MmBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(net(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b107ee2",
   "metadata": {},
   "source": [
    "# 4.5 读取和存储\n",
    "我们把内存中训练好的模型参数存储在硬盘上供后续读取使用\n",
    "## 4.5.1 读写tensor\n",
    "我们可以直接使⽤ save 函数和 load 函数分别存储和读取 Tensor 。 save 使⽤Python的pickle实⽤\n",
    "程序将对象进⾏序列化，然后将序列化的对象保存到disk，使⽤ save 可以保存各种对象,包括模型、张\n",
    "量和字典等。⽽ laod 使⽤pickle unpickle⼯具将pickle的对象⽂件反序列化为内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9940a3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "x = torch.ones(3)\n",
    "torch.save(x,'x.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "facea653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = torch.load('x.pt')\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2a8876eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1., 1., 1.]), tensor([0., 0., 0., 0.])]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y= torch.zeros(4)\n",
    "torch.save([x,y],'xy.pt')\n",
    "xy_list = torch.load('xy.pt')\n",
    "xy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c721f665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([1., 1., 1.]), 'y': tensor([0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save({'x':x,'y':y},'xy_dict.pt')\n",
    "xy = torch.load('xy_dict.pt')\n",
    "xy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f08b0c",
   "metadata": {},
   "source": [
    "## 4.5.2 读写模型\n",
    "state_dict 是⼀个从参数名称隐射到参数 Tesnor 的字典对象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "dad5a32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "     def __init__(self):\n",
    "         super(MLP, self).__init__()\n",
    "         self.hidden = nn.Linear(3, 2)\n",
    "         self.act = nn.ReLU()\n",
    "         self.output = nn.Linear(2, 1)\n",
    "     def forward(self, x):\n",
    "         a = self.act(self.hidden(x))\n",
    "         return self.output(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "526536a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('hidden.weight',\n",
       "              tensor([[-0.3664,  0.4260,  0.1871],\n",
       "                      [ 0.2061, -0.3556,  0.3939]])),\n",
       "             ('hidden.bias', tensor([-0.0040,  0.0298])),\n",
       "             ('output.weight', tensor([[0.4334, 0.6145]])),\n",
       "             ('output.bias', tensor([-0.2200]))])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NET  = MLP()\n",
    "NET.state_dict()\n",
    "#注意，只有具有可学习参数的层(卷积层、线性层等)\n",
    "# 才有 state_dict 中的条⽬。\n",
    "# 优化器( optim )也有⼀个 state_dict \n",
    "# 其中包含关于优化器状态以及所使⽤的超参数的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "69622c8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': {},\n",
       " 'param_groups': [{'lr': 0.001,\n",
       "   'momentum': 0.9,\n",
       "   'dampening': 0,\n",
       "   'weight_decay': 0,\n",
       "   'nesterov': False,\n",
       "   'params': [0, 1, 2, 3]}]}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(NET.parameters(),lr = 0.001,momentum = 0.9)\n",
    "optimizer.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2b7819",
   "metadata": {},
   "source": [
    "### 4.5.2.2 保存和加载模型\n",
    "#### 推荐保存和加载state_dict\n",
    "torch.save(model.state_dict(),PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e55726af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True],\n",
       "        [True]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(2,3)\n",
    "Y = NET(X)\n",
    "\n",
    "Path = \"./net.pth\"\n",
    "torch.save(NET.state_dict(),Path)\n",
    "\n",
    "net2 = MLP()\n",
    "net2.load_state_dict(torch.load(Path))\n",
    "Y2 = net2(X)\n",
    "Y2 == Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a552dfe",
   "metadata": {},
   "source": [
    "# 4.6 GPU计算\n",
    "到⽬前为⽌，我们⼀直在使⽤CPU计算。\n",
    "对复杂的神经⽹络和⼤规模的数据来说，使⽤CPU来计算可能不够⾼效。\n",
    "在本节中，我们将介绍如何使⽤单块NVIDIA GPU来计算。\n",
    "所以需要确保已经安装好了PyTorch GPU版本。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "122ccfff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'nvidia-smi' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44158b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "torch.cuda.is_available() # 输出 True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd1170b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count() # 输出 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ce4faf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device() # 输出 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70b27186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce GTX 1080'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0) # 输出 'GeForce GTX 1050'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31798403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 2, 3], device='cuda:0'), device(type='cuda', index=0))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1,2,3])\n",
    "x = x.cuda()\n",
    "x,x.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "deedaaa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 2, 3], device='cuda:0'), tensor([3, 4, 5], device='cuda:0'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "x = torch.tensor([1,2,3],device = device)\n",
    "\n",
    "y = torch.tensor([3,4,5]).to(device)\n",
    "x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2b27f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Linear(3,1)\n",
    "list(net.parameters())[0].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fdaede7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.cuda()\n",
    "list(net.parameters())[0].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99fca69f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4807],\n",
       "        [-0.3510]], device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x= torch.rand(2,3).cuda()\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fada59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
